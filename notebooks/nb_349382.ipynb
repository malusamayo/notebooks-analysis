{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 256 samples\n",
      "Epoch 1/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 1.5824Epoch 00000: val_loss improved from inf to 1.33989, saving model to weit/wave-adam-00-1.3399.hdf5\n",
      "5600/5600 [==============================] - 64s - loss: 1.5780 - val_loss: 1.3399\n",
      "Epoch 2/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 1.1712Epoch 00001: val_loss improved from 1.33989 to 1.00709, saving model to weit/wave-adam-01-1.0071.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 1.1692 - val_loss: 1.0071\n",
      "Epoch 3/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 1.0569Epoch 00002: val_loss improved from 1.00709 to 0.96682, saving model to weit/wave-adam-02-0.9668.hdf5\n",
      "5600/5600 [==============================] - 54s - loss: 1.0560 - val_loss: 0.9668\n",
      "Epoch 4/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 1.0024Epoch 00003: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 1.0029 - val_loss: 1.0324\n",
      "Epoch 5/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.9700Epoch 00004: val_loss improved from 0.96682 to 0.93873, saving model to weit/wave-adam-04-0.9387.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.9699 - val_loss: 0.9387\n",
      "Epoch 6/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.9510Epoch 00005: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.9529 - val_loss: 0.9492\n",
      "Epoch 7/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.9236Epoch 00006: val_loss improved from 0.93873 to 0.87887, saving model to weit/wave-adam-06-0.8789.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.9227 - val_loss: 0.8789\n",
      "Epoch 8/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.9088Epoch 00007: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.9082 - val_loss: 0.9162\n",
      "Epoch 9/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8993Epoch 00008: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.8987 - val_loss: 0.8964\n",
      "Epoch 10/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8761Epoch 00009: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.8760 - val_loss: 0.9129\n",
      "Epoch 11/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8689Epoch 00010: val_loss improved from 0.87887 to 0.83789, saving model to weit/wave-adam-10-0.8379.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.8687 - val_loss: 0.8379\n",
      "Epoch 12/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8584Epoch 00011: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.8579 - val_loss: 0.8404\n",
      "Epoch 13/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8475Epoch 00012: val_loss improved from 0.83789 to 0.83410, saving model to weit/wave-adam-12-0.8341.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.8469 - val_loss: 0.8341\n",
      "Epoch 14/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8370Epoch 00013: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.8376 - val_loss: 0.8445\n",
      "Epoch 15/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8241Epoch 00014: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.8262 - val_loss: 0.8755\n",
      "Epoch 16/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8181Epoch 00015: val_loss improved from 0.83410 to 0.81892, saving model to weit/wave-adam-15-0.8189.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.8178 - val_loss: 0.8189\n",
      "Epoch 17/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8078Epoch 00016: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.8077 - val_loss: 0.8722\n",
      "Epoch 18/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.8014Epoch 00017: val_loss improved from 0.81892 to 0.80630, saving model to weit/wave-adam-17-0.8063.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.8012 - val_loss: 0.8063\n",
      "Epoch 19/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7940Epoch 00018: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.7941 - val_loss: 0.8775\n",
      "Epoch 20/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7859Epoch 00019: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.7858 - val_loss: 0.8279\n",
      "Epoch 21/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7817Epoch 00020: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.7822 - val_loss: 0.8093\n",
      "Epoch 22/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7236Epoch 00021: val_loss improved from 0.80630 to 0.78486, saving model to weit/wave-adam-21-0.7849.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.7238 - val_loss: 0.7849\n",
      "Epoch 23/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7151Epoch 00022: val_loss improved from 0.78486 to 0.78446, saving model to weit/wave-adam-22-0.7845.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.7150 - val_loss: 0.7845\n",
      "Epoch 24/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7104Epoch 00023: val_loss improved from 0.78446 to 0.78373, saving model to weit/wave-adam-23-0.7837.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.7111 - val_loss: 0.7837\n",
      "Epoch 25/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7088Epoch 00024: val_loss improved from 0.78373 to 0.78371, saving model to weit/wave-adam-24-0.7837.hdf5\n",
      "5600/5600 [==============================] - 53s - loss: 0.7092 - val_loss: 0.7837\n",
      "Epoch 26/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7070Epoch 00025: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.7072 - val_loss: 0.7841\n",
      "Epoch 27/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7047Epoch 00026: val_loss improved from 0.78371 to 0.78353, saving model to weit/wave-adam-26-0.7835.hdf5\n",
      "5600/5600 [==============================] - 54s - loss: 0.7046 - val_loss: 0.7835\n",
      "Epoch 28/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7029Epoch 00027: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.7031 - val_loss: 0.7838\n",
      "Epoch 29/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7008Epoch 00028: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.7015 - val_loss: 0.7841\n",
      "Epoch 30/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.7000Epoch 00029: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.7000 - val_loss: 0.7844\n",
      "Epoch 31/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.6988Epoch 00030: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.6985 - val_loss: 0.7846\n",
      "Epoch 32/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.6969Epoch 00031: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.6970 - val_loss: 0.7848\n",
      "Epoch 33/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.6955Epoch 00032: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.6953 - val_loss: 0.7847\n",
      "Epoch 34/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.6940Epoch 00033: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.6941 - val_loss: 0.7850\n",
      "Epoch 35/100\n",
      "5520/5600 [============================>.] - ETA: 0s - loss: 0.6929Epoch 00034: val_loss did not improve\n",
      "5600/5600 [==============================] - 53s - loss: 0.6928 - val_loss: 0.7856\n",
      "Epoch 36/100\n",
      " 160/5600 [..............................] - ETA: 51s - loss: 0.6978"
     ]
    }
   ],
   "source": [
    "# (c) Copyright M. Rizky Luthfianto. 2016\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "try:\n",
    "    import pickle\n",
    "except:\n",
    "    import cPickle as pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.activations import hard_sigmoid\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, History\n",
    "\n",
    "onehot = np.arange(0,21)\n",
    "PSSM = np.arange(35,56)\n",
    "features = np.hstack((onehot, PSSM))\n",
    "labels = np.arange(22,30)\n",
    "\n",
    "cull_ = np.load('cullpdb+profile_6133.npy')\n",
    "cull = np.reshape(cull_, (-1, 700, 57))\n",
    "\n",
    "X_train    = cull[0:5600, :, features]\n",
    "y_train    = cull[0:5600, :, labels]\n",
    "train_mask = cull[0:5600, :, 30] * -1 + 1\n",
    "\n",
    "X_test    = cull[5605:5887, :, features]\n",
    "y_test    = cull[5605:5887, :, labels]\n",
    "test_mask = cull[5605:5887, :, 30] * -1 + 1\n",
    "\n",
    "X_val    = cull[5877:6133, :, features]\n",
    "y_val    = cull[5877:6133, :, labels]\n",
    "val_mask = cull[5877:6133, :, 30] * -1 + 1\n",
    "\n",
    "def masked_accuracy(y_true, y_pred, data_mask):\n",
    "    sames = np.equal(np.argmax(y_true, axis=-1), np.argmax(y_pred, axis=-1)) # is Array\n",
    "    filtered = sames * data_mask\n",
    "    return np.sum(filtered)/np.sum(data_mask)\n",
    "\n",
    "def masked_val_accuracy(y_true, y_pred):\n",
    "    return masked_accuracy(y_true, y_pred, val_mask)\n",
    "\n",
    "def masked_cb_accuracy(y_true, y_pred):\n",
    "    return masked_accuracy(y_true, y_pred, cb_mask)\n",
    "\n",
    "n_inputs = 42\n",
    "num_classes = 8\n",
    "seq_len = 700\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#l2_num=1\n",
    "#l2_num=0.1\n",
    "#l2_num=0.01\n",
    "#l2_num=0.001\n",
    "l2_num=0\n",
    "\n",
    "if l2_num==0:\n",
    "    l2l=None\n",
    "else:\n",
    "    l2l=l2(l2_num)\n",
    "\n",
    "from tensorflow_ops import MyAtrousConvolution1D, make_parallel\n",
    "from keras.layers import Activation, Convolution1D, Dense, Dropout, Input, merge\n",
    "from keras.layers import AtrousConvolution1D\n",
    "from keras.models import Model\n",
    "\n",
    "num_protein_features = 42\n",
    "num_filters = 100\n",
    "num_label=8\n",
    "\n",
    "init='he_normal'\n",
    "\n",
    "def WavenetBlock(n_atrous_filters, atrous_filter_size, atrous_rate):\n",
    "    def f(input_):\n",
    "        residual = input_\n",
    "        tanh_out    = MyAtrousConvolution1D(n_atrous_filters, atrous_filter_size, init=init,\n",
    "                                            atrous_rate=atrous_rate,\n",
    "                                            border_mode='same',# dim_ordering='tf',\n",
    "                                            activation='tanh')(input_)\n",
    "        sigmoid_out = MyAtrousConvolution1D(n_atrous_filters, atrous_filter_size, init=init,\n",
    "                                            atrous_rate=atrous_rate,\n",
    "                                            border_mode='same',#  dim_ordering='tf',\n",
    "                                            activation='sigmoid')(input_)\n",
    "        merged = merge([tanh_out, sigmoid_out], mode='mul')\n",
    "\n",
    "        skip_out = Convolution1D(num_protein_features, 1, activation='relu', border_mode='same', init=init)(merged)\n",
    "        out = merge([skip_out, residual], mode='sum')\n",
    "        return out, skip_out\n",
    "    return f\n",
    "\n",
    "def GluBlock(n_atrous_filters, atrous_filter_size, atrous_rate):\n",
    "    def f(input_):\n",
    "        residual = input_\n",
    "        linear_out  = MyAtrousConvolution1D(n_atrous_filters, atrous_filter_size, init=init,\n",
    "                                            atrous_rate=atrous_rate,\n",
    "                                            border_mode='same',# dim_ordering='tf',\n",
    "                                            activation='linear')(input_)\n",
    "        sigmoid_out = MyAtrousConvolution1D(n_atrous_filters, atrous_filter_size, init=init,\n",
    "                                            atrous_rate=atrous_rate,\n",
    "                                            border_mode='same',#  dim_ordering='tf',\n",
    "                                            activation='sigmoid')(input_)\n",
    "        merged = merge([linear_out, sigmoid_out], mode='mul')\n",
    "\n",
    "        skip_out = Convolution1D(num_protein_features, 1, activation='relu', border_mode='same', init=init)(merged)\n",
    "        out = merge([skip_out, residual], mode='sum')\n",
    "        return out, skip_out\n",
    "    return f\n",
    "\n",
    "num_timesteps = 700\n",
    "\n",
    "def create_model(gate='wave'):\n",
    "    input_ = Input(shape=(num_timesteps, num_protein_features))\n",
    "\n",
    "    if gate=='glu':\n",
    "        A, B = GluBlock(num_filters, atrous_filter_size=2, atrous_rate=2)(input_)        \n",
    "    elif gate=='wave':\n",
    "        A, B = WavenetBlock(num_filters, atrous_filter_size=2, atrous_rate=2)(input_)\n",
    "    \n",
    "    skip_connections = [B]\n",
    "    for i in range(30):\n",
    "        atrous_rate = 2**(i%10)\n",
    "        if gate=='glu':\n",
    "            A, B = GluBlock(num_filters, 2, atrous_rate)(A)\n",
    "        elif gate=='wave':\n",
    "            A, B = WavenetBlock(num_filters, 2, atrous_rate)(A)\n",
    "        skip_connections.append(B)\n",
    "\n",
    "    net = merge(skip_connections, mode='sum')\n",
    "    net = Activation('relu')(net)\n",
    "\n",
    "    net = Convolution1D(num_label, 1, activation='relu', init=init)(net)\n",
    "    net = Convolution1D(num_label, 1, init=init)(net)\n",
    "    #net = Dropout(0.5)(net)\n",
    "    net = Dense(num_label, activation='softmax', init=init)(net)\n",
    "    model = Model(input=input_, output=net)\n",
    "    return model\n",
    "\n",
    "model = create_model('wave')\n",
    "\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "optimizer = RMSprop(0.0025)\n",
    "\n",
    "#from weightnorm import AdamWithWeightnorm, data_based_init\n",
    "#optimizer = AdamWithWeightnorm(0.001)\n",
    "#data_based_init(model, X_train[:100])\n",
    "#print('test')\n",
    "model.compile(optimizer, 'categorical_crossentropy', sample_weight_mode=\"temporal\")\n",
    "model.save_weights('init_dense.hdf5')\n",
    "\n",
    "\n",
    "\n",
    "#model = make_parallel(model, 2)\n",
    "#model.compile(optimizer, 'categorical_crossentropy', sample_weight_mode=\"temporal\")\n",
    "\n",
    "history=History()\n",
    "\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import keras.backend as K\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch<=20:\n",
    "        K.set_value(model.optimizer.lr, 0.0025)\n",
    "    elif epoch<=21:\n",
    "        K.set_value(model.optimizer.lr, 0.0001)\n",
    "    return float(K.get_value(model.optimizer.lr))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=5, min_lr=0.000001, verbose=1)\n",
    "\n",
    "checkpointer = ModelCheckpoint('weit/wave-adam-{epoch:02d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1,\n",
    "                                save_best_only=True, save_weights_only=True)\n",
    "\n",
    "batch_size = 80#*2#16*2\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=100, verbose=1, sample_weight=train_mask,\n",
    "          callbacks=[checkpointer,\n",
    "                     lr_scheduler,\n",
    "                     history],\n",
    "          validation_data=(X_val, y_val, val_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(history.history, open('wev-100-12-history-schedule.p','wb'))\n",
    "#history2=pickle.load(open('wavenet8-history-schedule.p','rb'))\n",
    "\n",
    "model.save_weights('wev-100-12-last-epoch-100.hdf5')\n",
    "#model.load_weights('wavenet8-rmsprop-sched-93-0.7825.hdf5')\n",
    "\n",
    "#\n",
    "history2=history.history\n",
    "import matplotlib.pyplot as plt\n",
    "#get_ipython().magic(u'matplotlib inline')\n",
    "plt.plot(xrange(len(history2['loss'])), history2['loss'])\n",
    "plt.plot(xrange(len(history2['val_loss'])), history2['val_loss'])\n",
    "\n",
    "\n",
    "#\n",
    "cb_=np.load('cb513+profile_split1.npy')\n",
    "cb = np.reshape(cb_,(514,700,57))\n",
    "X_cb =cb[:,:, features]\n",
    "y_cb =cb[:,:, labels]\n",
    "cb_mask = cb[:,:,30] * -1 + 1\n",
    "\n",
    "\n",
    "ypcb=model.predict(X_cb, 64)\n",
    "masked_cb_accuracy(y_cb, ypcb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
