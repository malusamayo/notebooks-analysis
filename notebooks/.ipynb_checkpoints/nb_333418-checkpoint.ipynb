{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing modules\n",
    "#print backward compatibility\n",
    "from __future__ import print_function\n",
    "\n",
    "#For plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Mathematical operation on data\n",
    "import numpy as np\n",
    "\n",
    "#typical imports for data reading from local directory\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "#Used to extract tar files\n",
    "import tarfile\n",
    "\n",
    "#For displaying images on notebook\n",
    "from IPython.display import display, Image\n",
    "\n",
    "#For dealing with 2 dim images, coverting to pixel values\n",
    "from scipy import ndimage\n",
    "\n",
    "#For machine learning with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#For downloading dataset from a given url\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "#Saving memory data in file for future fast access (doesnot work for me)\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "#For dealing with 2D images\n",
    "import scipy.ndimage as spnd\n",
    "\n",
    "#for saving data struture in memory for faster future retrival (works for me)\n",
    "import pickle\n",
    "\n",
    "#For comparing structural similarity between two images\n",
    "#http://www.pyimagesearch.com/2014/09/15/python-compare-two-images/\n",
    "from skimage.measure import structural_similarity as ssim\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "\n",
    "\n",
    "\"\"\"Show interactive data downloading (notMNIST data image files)\"\"\"\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 1% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "\n",
    "\"\"\"Downloads data with error checking\"\"\"\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "#############################################################################################\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of classes we have with images are only 10, i.e. A to J\n",
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "\n",
    "\"\"\"After downloading the data, extract the tar file, and return folder names in list,\n",
    "    do not extract if already extracted\"\"\"\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "\n",
    "#############################################################################################\n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#image description\n",
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "\"\"\"For each letter, load the images in dataset and return dataset for pickling\"\"\"\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  #create the array to hold the images from one class\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      #imgaes has to be normalized for fast operation on ML techniques\n",
    "      image_data = (spnd.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "\n",
    "\"\"\"Read the image files after extracting in memory, and save it in pickle for \n",
    "faster retrival at later time (if not already saved in pickle format)\"\"\"\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "###################################################################################\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Have to create labels for imgaes, so that true answers can be feed to model to make it accurate\n",
    "characters = ['A','B','C','D','E','F','G','H','I','J']\n",
    "\n",
    "#Check if data has been loaded and getting printed\n",
    "fig=plt.figure()\n",
    "index=1\n",
    "for c in characters:\n",
    "    a=fig.add_subplot(1,10,index)\n",
    "    index = index+1\n",
    "    data = pickle.load(open(\"notMNIST_large/\"+c+\".pickle\",\"rb\"))\n",
    "    plt.imshow(data[random.randint(0,len(data)),:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Make space for new(merged) dataset along with space for labels\"\"\"\n",
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "\"\"\"Merge all letter dataset into one, but seperating out the validation set if required\"\"\"\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "####################################################################################\n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Randomise each training, validation and test set along with labels\"\"\"\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "##################################################################################\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check if randomization is goooood\n",
    "fig=plt.figure()\n",
    "index = 1\n",
    "for idx in range(0,10):\n",
    "    b=fig.add_subplot(1,10,index)\n",
    "    index = index+1\n",
    "    plt.imshow(train_dataset[index-1,:,:])\n",
    "    print(train_labels[index-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dump everything in pickle, for later fast retrival\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the pickle, Check for each training data if it is in validation or test. If yes, delete the data with coresponding \n",
    "#label in training data, save the training data combined with validation and testing into pickle in order to create a \n",
    "#sanitised set\n",
    "fig=plt.figure()\n",
    "index = 1\n",
    "\n",
    "#b=fig.add_subplot(1,10,index)\n",
    "#plt.imshow(train_dataset[index-1,:,:])\n",
    "\n",
    "dist = 0\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "with open(pickle_file,'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    #train_dataset, train_labels, valid_dataset , valid_labels, test_dataset, test_labels = pickle.load(f)\n",
    "    pickle_dict = pickle.load(f)\n",
    "    train_dataset = pickle_dict[\"train_dataset\"]\n",
    "    train_labels = pickle_dict[\"train_labels\"]\n",
    "    valid_dataset = pickle_dict[\"valid_dataset\"]\n",
    "    valid_labels = pickle_dict[\"valid_labels\"]\n",
    "    test_dataset = pickle_dict[\"test_dataset\"]\n",
    "    test_labels = pickle_dict[\"test_labels\"]\n",
    "    #print(train_dataset.shape[0])\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            dist = ssim(train_dataset[i, : , : ],valid_dataset[j,:,:])\n",
    "            print(dist)\n",
    "            if(dist>0.95):\n",
    "                b=fig.add_subplot(1,10,index)\n",
    "                plt.imshow(train_dataset[i,:,:])\n",
    "                plt.imshow(valid_dataset[i,:,:])\n",
    "                print(dist)\n",
    "                index = index+1\n",
    "                \n",
    "#the below number shows that structural similarity is very less atleas for 10 traing data, so assume each data is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "nr_training = [50,100,5000]\n",
    "\n",
    "#For logistic regression, dataset has to be reshaped\n",
    "train_dataset = train_dataset.reshape(len(train_dataset), -1)\n",
    "test_dataset = test_dataset.reshape(len(test_dataset), -1)\n",
    "\n",
    "for i in range(0,len(nr_training)):\n",
    "    print(\"With \"+str(nr_training[i])+\" training examples\")\n",
    "    model = LogisticRegression()\n",
    "    model = model.fit(train_dataset[0:nr_training[i]], train_labels[0:nr_training[i]])\n",
    "    #print(model.score(train_dataset[0:nr_training[i]], train_labels[0:nr_training[i]]))\n",
    "    #print(model.score(train_dataset[0:nr_training[i]], train_labels[0:nr_training[i]]))\n",
    "    predicted = model.predict(test_dataset)\n",
    "    print(metrics.accuracy_score(test_labels, predicted))\n",
    "#With larger number of data example, prediction accuracy increases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
